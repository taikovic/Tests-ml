{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Linear Regression:Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data load: \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing=fetch_california_housing()\n",
    "housing[\"data\"].shape # equal: housing.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block\\n        - HouseAge      median house age in block\\n        - AveRooms      average number of rooms\\n        - AveBedrms     average number of bedrooms\\n        - Population    block population\\n        - AveOccup      average house occupancy\\n        - Latitude      house block latitude\\n        - Longitude     house block longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttp://lib.stat.cmu.edu/datasets/\\n\\nThe target variable is the median house value for California districts.\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.target\n",
    "#!: tableau a 1 seule dim:=> le transformer en 1! vecteur colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]]\n"
     ]
    }
   ],
   "source": [
    "m,n=housing.data.shape\n",
    "housing_data_plus_bias=np.c_[np.ones((m,1)),housing.data] #Ajout de B0 sur ttes les lignes\n",
    "\n",
    "X=tf.constant(housing_data_plus_bias,dtype=tf.float32, name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "XT=tf.transpose(X)\n",
    "theta=tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)\n",
    "with tf.Session() as sess:\n",
    "    theta_val=theta.eval()\n",
    "print(theta_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Batch Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of data:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Scaler=StandardScaler()\n",
    "scaled_housing_data=Scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias=np.c_[np.ones((m,1)),scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE:  12.408011\n",
      "Epoch:  100 MSE:  0.755197\n",
      "Epoch:  200 MSE:  0.5420873\n",
      "Epoch:  300 MSE:  0.5331699\n",
      "Epoch:  400 MSE:  0.5305383\n",
      "Epoch:  500 MSE:  0.5287961\n",
      "Epoch:  600 MSE:  0.52754897\n",
      "Epoch:  700 MSE:  0.5266499\n",
      "Epoch:  800 MSE:  0.52600086\n",
      "Epoch:  900 MSE:  0.5255331\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "# Variables Build\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\") # cree 1 node\n",
    "\n",
    "#instruction Build\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred - y \n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "gradients=2/m * tf.matmul(tf.transpose(X),error)# Gradients calcules!!!!\n",
    "training_op=tf.assign(theta,theta - learning_rate * gradients) #assign function cree 1 node\n",
    "\n",
    "#passons a l'execution:\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: \",epoch,\"MSE: \",mse.eval())\n",
    "        training_op.eval()# OR, sess.run(training_op)\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523e+00],\n",
       "       [ 8.1063598e-01],\n",
       "       [ 1.2685776e-01],\n",
       "       [-2.0784086e-01],\n",
       "       [ 2.4839850e-01],\n",
       "       [-1.3083885e-03],\n",
       "       [-3.9607048e-02],\n",
       "       [-8.5861266e-01],\n",
       "       [-8.2600272e-01]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Automatic Differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "# Variables Build\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\") # cree 1 node\n",
    "\n",
    "#instruction Build\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred - y \n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient: fourni par tensorflow, vecteur a 9 composantes\n",
    "gradients=tf.gradients(mse,[theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE:  12.408011\n",
      "Epoch:  100 MSE:  0.75519687\n",
      "Epoch:  200 MSE:  0.5420873\n",
      "Epoch:  300 MSE:  0.5331699\n",
      "Epoch:  400 MSE:  0.5305383\n",
      "Epoch:  500 MSE:  0.5287961\n",
      "Epoch:  600 MSE:  0.52754897\n",
      "Epoch:  700 MSE:  0.52664983\n",
      "Epoch:  800 MSE:  0.52600086\n",
      "Epoch:  900 MSE:  0.5255331\n",
      "Best theta:\n",
      "[[ 2.0685525e+00]\n",
      " [ 8.1063598e-01]\n",
      " [ 1.2685777e-01]\n",
      " [-2.0784083e-01]\n",
      " [ 2.4839847e-01]\n",
      " [-1.3083883e-03]\n",
      " [-3.9607048e-02]\n",
      " [-8.5861266e-01]\n",
      " [-8.2600272e-01]]\n"
     ]
    }
   ],
   "source": [
    "training_op=tf.assign(theta,theta - learning_rate * gradients)\n",
    "#passons a l'execution:\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: \",epoch,\"MSE: \",mse.eval())\n",
    "        training_op.eval()# OR, sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - tensorflow GD Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "# Variables Build\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\") # cree 1 node\n",
    "\n",
    "#instruction Build\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred - y \n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition de l'objet fonction d'optimization:\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "# minimize objectif function: \n",
    "training_op=optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE:  12.408011\n",
      "Epoch:  100 MSE:  0.75519687\n",
      "Epoch:  200 MSE:  0.5420873\n",
      "Epoch:  300 MSE:  0.5331699\n",
      "Epoch:  400 MSE:  0.5305383\n",
      "Epoch:  500 MSE:  0.5287961\n",
      "Epoch:  600 MSE:  0.52754897\n",
      "Epoch:  700 MSE:  0.52664983\n",
      "Epoch:  800 MSE:  0.52600086\n",
      "Epoch:  900 MSE:  0.5255331\n",
      "Best theta:\n",
      "[[ 2.0685525e+00]\n",
      " [ 8.1063598e-01]\n",
      " [ 1.2685777e-01]\n",
      " [-2.0784083e-01]\n",
      " [ 2.4839847e-01]\n",
      " [-1.3083883e-03]\n",
      " [-3.9607048e-02]\n",
      " [-8.5861266e-01]\n",
      " [-8.2600272e-01]]\n"
     ]
    }
   ],
   "source": [
    "#passons a l'execution:\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: \",epoch,\"MSE: \",mse.eval())\n",
    "        sess.run(training_op) #!! plus de fonction eval().\n",
    "    best_theta=theta.eval()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - tensorflow momentum optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "# Variables Build\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\") # cree 1 node\n",
    "\n",
    "#instruction Build\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred - y \n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition de l'objet fonction d'optimization avec l'inertie:\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "# minimize objectif function: \n",
    "training_op=optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE:  12.408011\n",
      "Epoch:  100 MSE:  0.5252007\n",
      "Epoch:  200 MSE:  0.5243332\n",
      "Epoch:  300 MSE:  0.5243213\n",
      "Epoch:  400 MSE:  0.52432084\n",
      "Epoch:  500 MSE:  0.524321\n",
      "Epoch:  600 MSE:  0.5243211\n",
      "Epoch:  700 MSE:  0.52432096\n",
      "Epoch:  800 MSE:  0.52432096\n",
      "Epoch:  900 MSE:  0.52432096\n",
      "Best theta:\n",
      "[[ 2.068558  ]\n",
      " [ 0.8296182 ]\n",
      " [ 0.11875144]\n",
      " [-0.26552498]\n",
      " [ 0.3056947 ]\n",
      " [-0.00450307]\n",
      " [-0.03932622]\n",
      " [-0.89988816]\n",
      " [-0.8705434 ]]\n"
     ]
    }
   ],
   "source": [
    "#passons a l'execution:\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: \",epoch,\"MSE: \",mse.eval())\n",
    "        sess.run(training_op) #!! plus de fonction eval().\n",
    "    best_theta=theta.eval()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Substitution nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_val1: \n",
      "[[6. 7. 8.]]\n",
      "\n",
      "\n",
      "B_val2: \n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "# call placeholder function:\n",
    "A = tf.placeholder(tf.float32,shape=(None,3))\n",
    "B = A + 5\n",
    "# pour l'evaluation: on passe a eval() de B, un feed_dict qui precise la valeur de A\n",
    "with tf.Session():\n",
    "    B_val1=B.eval(feed_dict={A:[[1,2,3]]})\n",
    "    B_val2=B.eval(feed_dict={A:[[4,5,6],[7,8,9]]})\n",
    "print(\"B_val1: \")\n",
    "print(B_val1)\n",
    "print(\"\\n\")\n",
    "print(\"B_val2: \")\n",
    "print(B_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Mini-batch Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "learning_rate=0.01\n",
    "n_epochs=10\n",
    "batch_size=100 #ce qui fait epochs total:1000\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "# Variables Build\n",
    "X=tf.placeholder(tf.float32,shape=(None,n+1),name=\"X\")\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name=\"y\")\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\") # cree 1 node\n",
    "\n",
    "#instruction Build\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred - y \n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(mse)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_theta: \n",
      "[[ 2.0701346 ]\n",
      " [ 0.8322434 ]\n",
      " [ 0.11718354]\n",
      " [-0.25360975]\n",
      " [ 0.33584762]\n",
      " [ 0.00311589]\n",
      " [-0.01102609]\n",
      " [-0.90628386]\n",
      " [-0.8712015 ]]\n"
     ]
    }
   ],
   "source": [
    "def fetch_batch(epoch,batch_index,batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index) #eviter les memes lignes\n",
    "    indices=np.random.randint(m,size=batch_size)\n",
    "    X_batch=scaled_housing_data_plus_bias[indices]\n",
    "    y_batch=housing.target.reshape(-1,1)[indices]\n",
    "    return X_batch, y_batch\n",
    "#Execution: \n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch=fetch_batch(epoch,batch_index,batch_size) #input\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch}) #feed_dict: passer les inputs\n",
    "    best_theta=theta.eval() #theta n'a besoin de rien pour son evaluation;\n",
    "    \n",
    "print(\"Best_theta: \")\n",
    "print(best_theta)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Save/restore models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 MSE:  9.161543\n",
      "Epoch:  100 MSE:  0.7145006\n",
      "Epoch:  200 MSE:  0.566705\n",
      "Epoch:  300 MSE:  0.5555719\n",
      "Epoch:  400 MSE:  0.5488112\n",
      "Epoch:  500 MSE:  0.5436362\n",
      "Epoch:  600 MSE:  0.5396294\n",
      "Epoch:  700 MSE:  0.5365092\n",
      "Epoch:  800 MSE:  0.5340678\n",
      "Epoch:  900 MSE:  0.5321474\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                      \n",
    "learning_rate = 0.01                                                                  \n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            \n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            \n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      \n",
    "error = y_pred - y                                                                    \n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)           \n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "# Creation du Noeud Saver a la fin de la phase de construction\n",
    "# Apres la creation de tous les noeuds et variables !!!\n",
    "saver=tf.train.Saver() #ici, toutes vars et nodes sont sauvegardes:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0 :\n",
    "            print(\"Epoch: \",epoch,\"MSE: \",mse.eval())\n",
    "            save_path=saver.save(sess,\"./tmp/my_regmodel.ckpt\") #save every 100 iters\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "    save_path=saver.save(sess,\"./tmp/my_final_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.6614528 ],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/my_final_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Restore the model:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\"./tmp/my_final_model.ckpt\")\n",
    "    best_theta_restored=theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta,best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut preciser la variable a sauvegarder avec son nom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.train.Saver({\"weights\":theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet Saver enregistre aussi un graphe qui a le nom du fichier sauvegarder + l'extension .meta\n",
    "on peut le charger avec le graphe par defaut, comme suit: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/my_final_model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.6614528 ],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph() # graph vide mntnt\n",
    "saver=tf.train.import_meta_graph(\"./tmp/my_final_model.ckpt.meta\") # on load la structure du graph\n",
    "theta=tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    saver.restore(sess,\"./tmp/my_final_model.ckpt\") #restore l'etat du graph\n",
    "    best_theta_restored=theta.eval()\n",
    "\n",
    "best_theta_restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta,best_theta_restored)\n",
    "#on peut tjrs loader un model sans pour autant le code qui le build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 -  Visualisation du Graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run-{}\".format(root_logdir,now)\n",
    "\n",
    "                                                              \n",
    "learning_rate = 0.01    \n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "m,n=housing.data.shape\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "housing_data_plus_bias=np.c_[np.ones((m,1)),housing.data]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")            \n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      \n",
    "error = y_pred - y                                                                    \n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)           \n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_summary=tf.summary.scalar(\"MSE\",mse)\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_theta: \n",
      "[[ 2.070016  ]\n",
      " [ 0.8204561 ]\n",
      " [ 0.1173173 ]\n",
      " [-0.22739051]\n",
      " [ 0.3113402 ]\n",
      " [ 0.00353193]\n",
      " [-0.01126994]\n",
      " [-0.91643935]\n",
      " [-0.8795008 ]]\n"
     ]
    }
   ],
   "source": [
    "def fetch_batch(epoch,batch_index,batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index) #eviter les memes lignes\n",
    "    indices=np.random.randint(m,size=batch_size)\n",
    "    X_batch=scaled_housing_data_plus_bias[indices]\n",
    "    y_batch=housing.target.reshape(-1,1)[indices]\n",
    "    return X_batch, y_batch\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch=fetch_batch(epoch,batch_index,batch_size) #input\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "                step=epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str,step)\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch}) #feed_dict: passer les inputs\n",
    "    best_theta=theta.eval() #theta n'a besoin de rien pour son evaluation;\n",
    "    \n",
    "print(\"Best_theta: \")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## name scope: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01    \n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "m,n=housing.data.shape\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "housing_data_plus_bias=np.c_[np.ones((m,1)),housing.data]\n",
    "# normalization of data:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Scaler=StandardScaler()\n",
    "scaled_housing_data=Scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias=np.c_[np.ones((m,1)),scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition du name scope:\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y                                                                    \n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_theta: \n",
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)           \n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "mse_summary=tf.summary.scalar(\"MSE\",mse)\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "def fetch_batch(epoch,batch_index,batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index) #eviter les memes lignes\n",
    "    indices=np.random.randint(m,size=batch_size)\n",
    "    X_batch=scaled_housing_data_plus_bias[indices]\n",
    "    y_batch=housing.target.reshape(-1,1)[indices]\n",
    "    return X_batch, y_batch\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch=fetch_batch(epoch,batch_index,batch_size) #input\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "                step=epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str,step)\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch}) #feed_dict: passer les inputs\n",
    "    best_theta=theta.eval() #theta n'a besoin de rien pour son evaluation;\n",
    "    \n",
    "print(\"Best_theta: \")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n",
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)\n",
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modularite: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Penser au fonctions pour automatiser: \n",
    "# fonction de somme des tensors:\n",
    "reset_graph()\n",
    "def relu(X):\n",
    "    w_shape=(int(X.get_shape()[1]),1)\n",
    "    w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "    b=tf.Variable(0.0,name=\"bias\")\n",
    "    z=tf.add(tf.matmul(X,w),b,name=\"z\")\n",
    "    return tf.maximum(z,0.,name=\"relu\")\n",
    "\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus=[relu(X) for i in range(5)]\n",
    "output=tf.add_n(relus,name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer=tf.summary.FileWriter(\"logs/relu1\",tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING Name Scope:\n",
    "reset_graph()\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\") as scope:\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "        b=tf.Variable(0.0,name=\"bias\")\n",
    "        z=tf.add(tf.matmul(X,w),b,name=\"z\")\n",
    "        return tf.maximum(z,0.,name=\"relu\")\n",
    "\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus=[relu(X) for i in range(5)]\n",
    "output=tf.add_n(relus,name=\"output\")\n",
    "file_writer=tf.summary.FileWriter(\"logs/relu2\",tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\",reuse=True):\n",
    "\n",
    "        #reutiliser la var existante\n",
    "        threshold=tf.get_variable(\"threshold\")\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "        b=tf.Variable(0.0,name=\"bias\")\n",
    "        z=tf.add(tf.matmul(X,w),b,name=\"z\")\n",
    "        return tf.maximum(z,0.,name=\"relu\")\n",
    "\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "with tf.variable_scope(\"relu\",reuse=True):                                #Creation de la variable\n",
    "    threshold = tf.get_variable(\"threshold\",initializer=tf.constant(0.0))\n",
    "\n",
    "relus=[relu(X) for relu_index in range(5)]\n",
    "output=tf.add_n(relus,name=\"output\")\n",
    "file_writer=tf.summary.FileWriter(\"logs/relu3\",tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definir la variable thershold 1!!! seule fois dans la fonction ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If initializer is a constant, do not specify shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-9f41925d2950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrelu_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelu_index\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mrelus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"output\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mfile_writer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"logs/relu5\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-9f41925d2950>\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"threshold\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mw_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taia006\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1485\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taia006\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1235\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1237\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mc:\\users\\taia006\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    538\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mc:\\users\\taia006\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taia006\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    844\u001b[0m       \u001b[0minitializing_from_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"If initializer is a constant, do not specify shape.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: If initializer is a constant, do not specify shape."
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\",reuse=True):\n",
    "        threshold=tf.get_variable(\"threshold\",initializer=tf.constant(0.0))\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "        b=tf.Variable(0.0,name=\"bias\")\n",
    "        z=tf.add(tf.matmul(X,w),b,name=\"z\")\n",
    "        return tf.maximum(z,0.,name=\"relu\")\n",
    "\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "file_writer=tf.summary.FileWriter(\"logs/relu5\",tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
